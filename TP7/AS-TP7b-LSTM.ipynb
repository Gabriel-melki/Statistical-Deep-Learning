{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "def code2char(code, vocab):\n",
    "    vocab_map = dict(zip(vocab.values(), vocab.keys()))\n",
    "    return \"\".join(vocab_map[c] for c in code)\n",
    "\n",
    "def char2code(text, vocab):\n",
    "    data = torch.ByteTensor(len(text))\n",
    "    for i, c in enumerate(text):\n",
    "        data[i] = vocab[c]\n",
    "    return data\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Charge un fichier data_file tenseur 1D d'entiers et le decoupe en sequences de longueur seq_length. Vocab_file est un dictionnaire\n",
    "        de characteres vers entier.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file, vocab_file, seq_length_entry, seq_length_exit):\n",
    "        self.data_file = data_file\n",
    "        self.vocab_file = vocab_file\n",
    "        self.seq_length_entry = seq_length_entry\n",
    "        self.seq_length_exit = seq_length_exit\n",
    "        self.data = torch.load(data_file)\n",
    "        self.vocab = torch.load(vocab_file)\n",
    "        self.vocab_map = dict(zip(self.vocab.values(), self.vocab.keys()))\n",
    "        self.nb_samples = len(self.data) - self.seq_length_exit - self.seq_length_entry +1\n",
    "        #print('cutting off end of data so that the batches/sequences divide evenly')\n",
    "        #self.data = self.data[:(self.seq_length_exit * self.nb_samples + 1)]\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #index = min(index, len(self.data) - self.seq_length_exit - self.seq_length_entry )\n",
    "        start, end = index, index +  self.seq_length_entry\n",
    "        return self.data[start:end], self.data[end:(end + self.seq_length_exit)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nb_samples\n",
    "\n",
    "\n",
    "\n",
    "def make_files(text_file, out_tensorfile=None, out_vocabfile=None, vocab_file=None):\n",
    "    \"\"\" Permet de creer a partir d'un fichier texte le tenseur 1D encode en entier et le mapping\n",
    "    des caracteres vers leur code.\n",
    "    * text_file : le fichier texte brut\n",
    "    * out_tensorfile : si specifie, sauve le tenseur 1D de sortie\n",
    "    * out_vocabfile : si specifie, sauve le mapping entre caractere et code\n",
    "    * vocab_file : si specifie, charge le mapping entre caractere et code\n",
    "    \"\"\"\n",
    "    #with open(text_file) as f:\n",
    "    #    text = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', f.read().lower())\n",
    "    #    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = re.split('\\W+', text_file)\n",
    "\n",
    "    if vocab_file is not None:\n",
    "        vocab = torch.load(vocab_file)\n",
    "    else:\n",
    "        chars = set(text)\n",
    "        chars.add(' ')\n",
    "        chars.add('.')\n",
    "        chars.add(',')\n",
    "        vocab = dict(zip(sorted(chars), range(len(chars))))\n",
    "    data = char2code(text, vocab)\n",
    "    if out_vocabfile is not None:\n",
    "        torch.save(vocab, out_vocabfile)\n",
    "    if out_tensorfile is not None:\n",
    "        torch.save(data, out_tensorfile)\n",
    "    return data, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy.random as rdn\n",
    "\n",
    "\n",
    "class Neural_net(nn.Module):\n",
    "    def __init__(self, num_layers, size_list, f):\n",
    "        super(Neural_net, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.linear = nn.ModuleList([nn.Linear(size_list[i], size_list[i + 1]) for i in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in range(self.num_layers):\n",
    "            x = self.f(self.linear[layer](x))\n",
    "\n",
    "        return x;\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, size_h, size_dict, f):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.w1 = nn.Linear(size_dict, size_h)\n",
    "        self.w2 = nn.Linear(size_h, size_h)\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, h, x_i):\n",
    "        result = self.f(self.w1(x_i) + self.w2(h))\n",
    "        return result;\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, size_h, size_dict):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.w1 = nn.Linear(size_h, size_dict)\n",
    "\n",
    "    def forward(self, h):\n",
    "        result = F.softmax(self.w1(h))\n",
    "        return result;\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, size_x, size_y, size_dict, size_h, f):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.size_x = size_x\n",
    "        self.size_y = size_y\n",
    "        self.size_h = size_h\n",
    "        self.size_dict = size_dict\n",
    "        self.f = f\n",
    "\n",
    "        self.e1 = Encoder(self.size_h, self.size_dict, self.f)\n",
    "        self.e2 = Encoder(self.size_h, self.size_dict, self.f)\n",
    "        self.d = Decoder(self.size_h, self.size_dict)\n",
    "\n",
    "        self.mod_x_1 = nn.Linear(self.size_dict, self.size_h)\n",
    "        self.mod_x_2 = nn.Linear(self.size_dict, self.size_h)\n",
    "        self.mod_x_3 = nn.Linear(self.size_dict, self.size_h)\n",
    "        self.mod_x_4 = nn.Linear(self.size_dict, self.size_h)\n",
    "\n",
    "        self.mod_h_1 = nn.Linear(self.size_h, self.size_h)\n",
    "        self.mod_h_2 = nn.Linear(self.size_h, self.size_h)\n",
    "        self.mod_h_3 = nn.Linear(self.size_h, self.size_h)\n",
    "        self.mod_h_4 = nn.Linear(self.size_h, self.size_h)\n",
    "\n",
    "    def forward(self, h, c, mat_xv, mat_yv):\n",
    "        out = []\n",
    "        for i in range(self.size_x):\n",
    "            f = F.sigmoid(self.mod_x_1.forward(mat_xv[:, i]) + self.mod_h_1.forward(h))\n",
    "            a = F.sigmoid(self.mod_x_2.forward(mat_xv[:, i]) + self.mod_h_2.forward(h))\n",
    "            t = F.tanh(self.mod_x_3.forward(mat_xv[:, i]) + self.mod_h_3.forward(h))\n",
    "\n",
    "            c = c.mul(f) + a.mul(t)\n",
    "            h = F.sigmoid(self.mod_x_2.forward(mat_xv[:, i]) + self.mod_h_2.forward(h)).mul(F.tanh(c))\n",
    "\n",
    "        for j in range(self.size_y):\n",
    "            out.append(self.d.forward(h.view(-1)))\n",
    "            h = self.e2.forward(h.view(-1), mat_yv[:, j])\n",
    "        return out;\n",
    "\n",
    "    def forward2(self, h, c, mat_xv):\n",
    "        out2 = []\n",
    "        for i in range(self.size_x):\n",
    "            f = F.sigmoid(self.mod_x_1.forward(mat_xv[:, i]) + self.mod_h_1.forward(h))\n",
    "            a = F.sigmoid(self.mod_x_2.forward(mat_xv[:, i]) + self.mod_h_2.forward(h))\n",
    "            t = F.tanh(self.mod_x_3.forward(mat_xv[:, i]) + self.mod_h_3.forward(h))\n",
    "\n",
    "            c = c.mul(f) + a.mul(t)\n",
    "            h = F.sigmoid(self.mod_x_2.forward(mat_xv[:, i]) + self.mod_h_2.forward(h)).mul(F.tanh(c))\n",
    "\n",
    "        for j in range(self.size_y):\n",
    "            proba = self.d.forward(h.view(-1))\n",
    "            pred = torch.multinomial(proba, 1, replacement=True)[0]\n",
    "            out2.append(pred.data.numpy()[0])\n",
    "            # tirage par rapport à\n",
    "            h = self.e2.forward(h.view(-1), proba)\n",
    "        return out2;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import numpy.random as rdn\n",
    "import re\n",
    "\n",
    "# PARAMETERS\n",
    "T = 4  # taille d'entrée\n",
    "size_h = 55  # taille de l'espace latent\n",
    "batch_size = 1\n",
    "n = 1  # taille de sortie\n",
    "\n",
    "text_file = \"/Users/melkigabriel/Desktop/textforrnn.txt.rtf\"\n",
    "\n",
    "with open(text_file) as f:\n",
    "    text_test = re.sub(r'[^a-zA-Z0-9_\\s]', ' ', f.read().lower())\n",
    "    text_test = re.sub(r'\\s+', ' ', text_test)\n",
    "text_test=text_test[254:]   \n",
    "bobo = make_files(text_test, \"mon_tenseur.pt\", \"mon_vocab.pt\")\n",
    "\n",
    "V = len(bobo[1].keys())  # taille du dico\n",
    "\n",
    "cdset = CharDataset(\"mon_tenseur.pt\", \"mon_vocab.pt\", T, n)\n",
    "\n",
    "dataload = DataLoader(cdset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss = nn.NLLLoss()  # on rentre un logSoftMax\n",
    "\n",
    "model = LSTM(size_x=T, size_y=n, size_dict=V, size_h=size_h, f=F.tanh)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Variable containing:\n",
      " 764.2690\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 196.4538\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 44.2542\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 12.3192\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 4.5499\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 2.1619\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 1.2099\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 0.7508\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 0.5176\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n",
      "=======================\n",
      "Variable containing:\n",
      " 0.3875\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "informationsincerelygabrielmelki\n",
      "\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "#APPRENTISSAGE\n",
    "a = time.time()\n",
    "i=0\n",
    "\n",
    "err =Variable(torch.ones(1))\n",
    "while(time.time() - a < 3*60):\n",
    "    optimizer.zero_grad()\n",
    "    err = 0\n",
    "    for x_train, y_train in dataload:\n",
    "        mat_x_train = torch.FloatTensor(V, T).fill_(0)\n",
    "        for j in range(T):\n",
    "            mat_x_train[x_train[0][j] - 1, j] = 1\n",
    "        mat_x_train = Variable(mat_x_train)\n",
    "\n",
    "        y_train = y_train.type(torch.ByteTensor)\n",
    "\n",
    "        mat_y_train = torch.FloatTensor(V, n).fill_(0)\n",
    "        for j in range(n):\n",
    "            mat_y_train[y_train[0][j] - 1, j] = 1\n",
    "        mat_y_train = Variable(mat_y_train)\n",
    "\n",
    "        #y_train = Variable(y_train[0])\n",
    "\n",
    "        h = Variable(torch.zeros(size_h, x_train.size()[0]).view(-1))\n",
    "        c = Variable(torch.zeros(size_h, x_train.size()[0]).view(-1))\n",
    "    \n",
    "        out = model.forward(h,c, mat_x_train, mat_y_train)\n",
    "\n",
    "        for j in range(n):\n",
    "            err += loss.forward(torch.log(out[j].view(1, V)), Variable(y_train[0])[j].type(torch.LongTensor) )\n",
    "    if(i%(int(dataload.dataset.data.size()[0]/100))==0):\n",
    "        print(\"=======================\")\n",
    "        print(err)\n",
    "        print(code2char(x_train.view(-1),cdset.vocab))\n",
    "        print(code2char(y_train.view(-1),cdset.vocab))\n",
    "        print([code2char(torch.max(out[i].data,-1)[1],cdset.vocab) for i in range(n)])            \n",
    "    i +=1\n",
    "    err.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operationsinhochi\n",
      "minh\n",
      "minh\n",
      "minh\n",
      "43 43\n",
      "=============\n",
      "fieldwhichrequiresan\n",
      "aptitude\n",
      "aptitude\n",
      "aptitude\n",
      "86 86\n",
      "=============\n",
      "alsoshownmyskills\n",
      "in\n",
      "in\n",
      "in\n",
      "129 129\n",
      "=============\n",
      "therightfeaturesand\n",
      "ratio\n",
      "ratio\n",
      "ratio\n",
      "172 172\n",
      "=============\n",
      "tripinisraelfocused\n",
      "on\n",
      "on\n",
      "on\n",
      "215 215\n",
      "=============\n",
      "axaheadquartersraising25k\n",
      "currently\n",
      "currently\n",
      "currently\n",
      "258 258\n",
      "=============\n",
      "ofthe250accounts\n",
      "to\n",
      "to\n",
      "to\n",
      "301 301\n",
      "=============\n",
      "workinginfirm\n",
      "within\n",
      "within\n",
      "within\n",
      "344 344\n",
      "=============\n",
      "overviewofmanycultures\n",
      "and\n",
      "and\n",
      "and\n",
      "386 387\n",
      "=============\n",
      "0.997663551402\n"
     ]
    }
   ],
   "source": [
    "##ACCURACY ON TRAIN\n",
    "acc = 0\n",
    "k = 0\n",
    "for x_train, y_train in dataload:\n",
    "    mat_x_train = torch.FloatTensor(V, T).fill_(0)\n",
    "    for j in range(T):\n",
    "        mat_x_train[x_train[0][j] - 1, j] = 1\n",
    "    mat_x_train = Variable(mat_x_train)\n",
    "\n",
    "    mat_y_train = torch.FloatTensor(V, n).fill_(0)\n",
    "    for j in range(n):\n",
    "        mat_y_train[y_train[0][j] - 1, j] = 1\n",
    "    mat_y_train = Variable(mat_y_train)\n",
    "    h = Variable(torch.zeros(size_h, x_train.size()[0]).view(-1))\n",
    "    c = Variable(torch.zeros(size_h, x_train.size()[0]).view(-1))\n",
    "    out2 = model.forward2(h, c, mat_x_train)\n",
    "    out = model.forward(h, c, mat_x_train,mat_y_train)\n",
    "    acc += 1 * (out2[0] == y_train[0, 0])\n",
    "\n",
    "    k += 1\n",
    "    if(k%(int(dataload.dataset.data.size()[0]/10))==0):\n",
    "        print(code2char(x_train.view(-1), cdset.vocab))\n",
    "        print(code2char(y_train.view(-1), cdset.vocab))\n",
    "        print(code2char(out2, cdset.vocab))\n",
    "        print(code2char(torch.max(out[0].data, -1)[1], cdset.vocab))\n",
    "        print(acc, k)\n",
    "        print('=============')\n",
    "print(acc / k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
